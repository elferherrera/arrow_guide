<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Apache Arrow [Rust]</title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        
        <link rel="stylesheet" href="css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="introduction.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="arrays.html"><strong aria-hidden="true">2.</strong> Arrays</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="arrays_buffer.html"><strong aria-hidden="true">2.1.</strong> The buffer</a></li><li class="chapter-item expanded "><a href="arrays_data.html"><strong aria-hidden="true">2.2.</strong> Array data</a></li><li class="chapter-item expanded "><a href="arrays_primitive.html"><strong aria-hidden="true">2.3.</strong> Primitive array</a></li><li class="chapter-item expanded "><a href="arrays_nested.html"><strong aria-hidden="true">2.4.</strong> Nested arrays</a></li><li class="chapter-item expanded "><a href="arrays_recordbatch.html"><strong aria-hidden="true">2.5.</strong> RecordBatch</a></li><li class="chapter-item expanded "><a href="arrays_operations.html"><strong aria-hidden="true">2.6.</strong> Arrow Kernels</a></li></ol></li><li class="chapter-item expanded "><a href="ipc_intro.html"><strong aria-hidden="true">3.</strong> Arrow IPC</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="ipc_flatbuffers.html"><strong aria-hidden="true">3.1.</strong> Detour: FlatBuffers</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.2.</strong> Sharing data</div></li></ol></li><li class="chapter-item expanded "><a href="reading_files.html"><strong aria-hidden="true">4.</strong> Reading files</a></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">4.1.</strong> Reading CSV files</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">4.2.</strong> Reading JSON files</div></li><li class="chapter-item expanded "><a href="reading_parquet.html"><strong aria-hidden="true">4.3.</strong> Reading Parquet files</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">5.</strong> Arrow Flight</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">5.1.</strong> Setting up a Server</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">6.</strong> DataFusion</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">6.1.</strong> Query data</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">7.</strong> Ballista</div></li><li class="spacer"></li><li class="chapter-item expanded "><a href="contributors.html"><strong aria-hidden="true">8.</strong> Contributors</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">Apache Arrow [Rust]</h1>

                    <div class="right-buttons">
                        
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#introduction" id="introduction">Introduction</a></h1>
<p>Welcome to the Apache Arrow guide for the Rust programming language.  This
guide was created to help you become familiar with the Arrow crate and its
functionalities. </p>
<h2><a class="header" href="#what-is-apache-arrow" id="what-is-apache-arrow">What is Apache Arrow?</a></h2>
<p>According to its <a href="https://arrow.apache.org">website</a> Apache Arrow is defined
as:</p>
<blockquote>
<p>A language-independent columnar memory format for flat and hierarchical data,
organized for efficient analytic operations on modern hardware like CPUs and
GPUs. The Arrow memory format also supports zero-copy reads for
lightning-fast data access without serialization overhead.</p>
</blockquote>
<p>After reading the description you have probably come to the conclusion that
Apache Arrow sounds great and that it will give anyone working with data enough
tools to improve a data processing workflow.  But that's the catch, on its own
Apache Arrow is not an application or library that can be installed and used.
The objective of Apache Arrow is to define a set of specifications that need to
be followed by an implementation in order to allow:</p>
<ol>
<li>fast in-memory data access</li>
<li>sharing and zero copy of data between processes</li>
</ol>
<h3><a class="header" href="#fast-in-memory-data-access" id="fast-in-memory-data-access">Fast in-memory data access</a></h3>
<p>Apache Arrow allows fast memory access by defining its <a href="https://arrow.apache.org/overview/">in-memory columnar
format</a>. This columnar format defines a
standard and efficient in-memory representation of various datatypes, plain or
nested
(<a href="https://github.com/apache/arrow/blob/master/docs/source/format/Columnar.rst">reference</a>).</p>
<p>In other words, the Apache Arrow project has created a series of rules or
specifications to define how a datatype (int, float, string, list, etc.) is
stored in memory. Since the objective of the project is to store large amounts
of data in memory for further manipulation or querying, it uses a columnar data
definition. This means that when a dataset (data defined with several columns)
is stored in memory, it no longer maintains its rows representation but it is
changed to a columnar representation.</p>
<p>For example, lets say we have a dataset that is defined with three columns
named: <em>session_id</em>, <em>timestamp</em> and <em>source_id</em> (image below). Traditionally,
this file should be represented in memory maintaining its row representation
(image below, left). This means that the fields representing a row would be kept
next to each other. This makes memory management harder to achieve because there
are different datatypes next to each other; in this case a long, a date and a
string. Each of these datatypes will have different memory requirements (for
example, 8 bytes, 16 bytes or 32 bytes).</p>
<p align="center">
  <img src="images/simd.png">
</p>
<p>By changing the in memory representation of the file to a columnar form (image
above, right), the in-memory arrangement of the data becomes more efficient.
Similar datatypes are stored next to each other, making the access and columnar
querying faster to perform.</p>
<h3><a class="header" href="#sharing-data-between-processes" id="sharing-data-between-processes">Sharing data between processes</a></h3>
<p>Imagine a typical workflow for a data engineer. There is a process that is
producing data that belongs to a service monitoring the performance of a sales
page.  This data has to be read, processed and stored. Probably the engineer
would first set a script that is reading the data and storing the result in a
CSV or Parquet file. Then the engineer would need to create a pipeline to read
the file and transfer the data to a database. Once the data is stored some
analysis is needed to be done on the data, maybe Pandas is used to read the data
and extract information. Or, perhaps Spark is used to create a pipeline that
reads the database in order to create a stream of data to feed a dashboard. The
copy and convert process may end up looking like this:</p>
<p align="center">
  <img src="images/copy.png">
</p>
<p>As it can be seen, the data is copied and converted several times. This happens
every time a process needs to query the data. </p>
<p>By using a standard that all languages and processes can understand, the data
doesn't need to be copied and converted. There can be a single in-memory data
representation that can be used to feed all the required processes. The data
sharing can be done regarding the language that is used.</p>
<p align="center">
  <img src="images/shared.png">
</p>
<p>And thanks to this standardization the data can also be shared with processes
that don't share the same memory. By creating a data server, packets of data
with known structure (RecordBatch) can be sent across computers (or pods) and
the receiving process doesn't need to spend time coding and decoding the data
to a known format. The data is ready to be used once its being received.</p>
<p align="center">
  <img src="images/recordbatch.png">
</p>
<h2><a class="header" href="#the-rust-arrow-crate" id="the-rust-arrow-crate">The Rust Arrow crate</a></h2>
<p>These and other collateral benefits can only be achieved thanks to the work done
by the people collaborating in the Apache Arrow project. By looking at the
project <a href="https://github.com/apache/arrow">github page</a>, there are libraries for
the most common languages used today, and that includes Rust.</p>
<p>The Rust Arrow crate is a collection of structs and implementations that define
all the elements required to create Arrow arrays that follow the Apache Arrow
specification. In the next sections the basic blocks for working with the
crate will be discussed, providing enough examples to give you familiarity
to construct, share and query Arrow arrays.</p>
<h1><a class="header" href="#arrays" id="arrays">Arrays</a></h1>
<p>The Array is the center piece of the Rust Apache Arrow implementation. An array
is defined by different pieces of data and metadata, as it can be seen in the
next image.</p>
<p align="center">
  <img src="images/layout.png">
</p>
<p>From the image it can be seen that an Array is composed of one or more buffers,
a validity bitmap and a datatype definition. By using an Arrow Array, you can
map complex or nested data structures into memory, and with the data ordered
and loaded you can shared it across several processes using a RecordBatch. </p>
<p>In Rust, the Array trait is the building block for all the available types of 
data containers. These include:</p>
<ul>
<li>
<p><a href="https://docs.rs/arrow/3.0.0/arrow/array/type.BinaryArray.html">BinaryArray</a>
An array where each element is a byte whose maximum length is represented by a
i32. </p>
</li>
<li>
<p><a href="https://docs.rs/arrow/3.0.0/arrow/array/struct.DictionaryArray.html">DictionaryArray</a>
A dictionary array where each element is a single value indexed by an integer
key</p>
</li>
<li>
<p><a href="https://docs.rs/arrow/3.0.0/arrow/array/struct.FixedSizeBinaryArray.html">FixedSizeBinaryArray</a>
A type of FixedSizeListArray whose elements are binaries.</p>
</li>
<li>
<p><a href="https://docs.rs/arrow/3.0.0/arrow/array/struct.FixedSizeBinaryArray.html">FixedSizeListArray</a>
A type of FixedSizeListArray whose elements are binaries.</p>
</li>
<li>
<p><a href="https://docs.rs/arrow/3.0.0/arrow/array/type.LargeBinaryArray.html">LargeBinaryArray</a>
An array where each element is a byte whose maximum length is represented by a
i64.</p>
</li>
<li>
<p><a href="https://docs.rs/arrow/3.0.0/arrow/array/type.LargeListArray.html">LargeListArray</a>
A list array where each element is a variable-sized sequence of values with
the same type whose memory offsets between elements are represented by a i64.</p>
</li>
<li>
<p><a href="https://docs.rs/arrow/3.0.0/arrow/array/type.LargeStringArray.html">LargeStringArray</a>
An array where each element is a variable-sized sequence of bytes representing
a string whose maximum length (in bytes) is represented by a i64.</p>
</li>
<li>
<p><a href="https://docs.rs/arrow/3.0.0/arrow/array/type.ListArray.html">ListArray</a> A
list array where each element is a variable-sized sequence of values with the
same type whose memory offsets between elements are represented by a i32.</p>
</li>
<li>
<p><a href="https://docs.rs/arrow/3.0.0/arrow/array/struct.PrimitiveArray.html">PrimitiveArray</a>
Array whose elements are of primitive types.</p>
</li>
<li>
<p><a href="https://docs.rs/arrow/3.0.0/arrow/array/type.StringArray.html">StringArray</a>
An array where each element is a variable-sized sequence of bytes representing
a string whose maximum length (in bytes) is represented by a i32.</p>
</li>
<li>
<p><a href="https://docs.rs/arrow/3.0.0/arrow/array/struct.StructArray.html">StructArray</a>
A nested array type where each child (called field) is represented by a
separate array.</p>
</li>
<li>
<p><a href="https://docs.rs/arrow/3.0.0/arrow/array/struct.NullArray.html">NullArray</a> An
Array where all elements are nulls</p>
</li>
<li>
<p><a href="https://docs.rs/arrow/3.0.0/arrow/array/struct.UnionArray.html">UnionArray</a>
An Array that can represent slots of varying types.</p>
</li>
</ul>
<p>Each of these containers follow a set of rules in order to define some sort of
behaviour. For example, a PrimitiveArray is made out of elements of the same
datatype and it contains one data buffer and one validity buffer. Or a
StructArray is a nested Array containing child fields that represent separate
PrimitiveArrays. By using a combination of these arrays the user
is capable of storing a variety of data in memory. </p>
<blockquote>
<p><strong>Tip</strong>. To have a better idea of the components that make each of the
mentioned arrays and how they work together have a look at this
<a href="https://github.com/apache/arrow/blob/master/docs/source/format/Columnar.rst#physical-memory-layout">section</a>
of the columnar format. </p>
</blockquote>
<p>Given the different components that define an array, it is important to
understand the basic unit that allocates the required memory used to hold data;
the buffer.</p>
<h1><a class="header" href="#the-arrow-buffer" id="the-arrow-buffer">The Arrow Buffer</a></h1>
<p>The <a href="https://docs.rs/arrow/3.0.0/arrow/buffer/struct.Buffer.html">Buffer</a> is the
main data container in the Arrow Array. Depending on the type of array that is
being created, it can have one or many buffers holding information. So, this
means that an array could include a combination of a values buffer, a validity
bitmap buffer and an offset buffer.</p>
<p>However, all buffers are the same. A buffer is the representation of a
continuous memory region that is used to store data in memory. According to the
Arrow specification a buffer should be aligned and padded using multiples of 8
or 64 bytes.</p>
<p>To see how a buffer looks in Rust lets create one.</p>
<pre><pre class="playground"><code class="language-rust">use arrow::buffer::Buffer;

fn main() {
    let buffer_u8 = Buffer::from(&amp;[0u8, 1, 2, 3, 4, 5]);
    println!(&quot;{:?}&quot;, buffer_u8);
}
</code></pre></pre>
<blockquote>
<p><strong>Note</strong>. Don't use the &quot;Run this code&quot; button. The Arrow crate is not loaded
in the book and it will produce an error</p>
</blockquote>
<blockquote>
<p><strong>Tip</strong>. If you use <strong>&quot;{:#?}&quot;</strong> in the println! macro you should see a
formated version of the struct in your screen</p>
</blockquote>
<p>If you printed the previous code you should see something like this:</p>
<pre><code class="language-json">Buffer { 
    data: Bytes { 
        ptr: 0x1dcab5b5400, 
        len: 6,
        data: [0, 1, 2, 3, 4, 5] 
    }, 
    offset: 0 
}
</code></pre>
<p>As it can be seen, a buffer is made out of a Bytes structure and an offset. The
Bytes structure is used to represent the data in memory by using a pointer, the
number of elements it has, and the data itself. The offset is used by the arrays
to indicate an offset for reading the stored values. By creating a buffer the
constructor has allocated in memory enough bytes to store the supplied values
and it has given a pointer to access the stored data. It should also be noted
that the resulting buffer is immutable.</p>
<p>The normal usage of the Arrays don't require you to use pointer arithmetic to
access the data in the buffer, but as a learning experience lets use the pointer
to access the data in memory.</p>
<pre><pre class="playground"><code class="language-rust">use arrow::buffer::Buffer;

fn main() {
    let buffer_u8 = Buffer::from(&amp;[0u8, 1, 2, 3, 4, 5]);
    
    unsafe {
        for i in 0..5 {
            println!(&quot;{}&quot;, *buffer_u8.as_ptr().add(i));
        }
    }
}
</code></pre></pre>
<p>If you are following the examples, you should see printed the values 0 to 5 in
you screen. </p>
<p>Now lets change the type of elements the buffer is holding to u32 and see what
happens to the buffer.</p>
<pre><pre class="playground"><code class="language-rust">use arrow::buffer::Buffer;
use arrow::datatypes::ToByteSlice;

fn main() {
    let buffer_u32 = Buffer::from(&amp;[0u32, 1, 2, 3, 4, 5].to_byte_slice());
    
    println!(&quot;{:?}&quot;, buffer_u32);
}
</code></pre></pre>
<p>In this case a new element is introduced to the code; the <strong>ToByteSlice</strong> trait.
The ToByteSlice trait exposes the method to_byte_slice for <em>[T]</em> and <em>T</em> which
allows us to allocate the required memory using u8 as the base unit. This means
that now each <em>u32</em> number will be represented by four <em>u8</em> numbers. That can be
seen better by printing the new buffer:</p>
<pre><code class="language-json">Buffer { 
    data: Bytes { 
        ptr: 0x1ad7d5ffb00,
        len: 24,
        data: [0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 3, 0, 0, 0, 4, 0, 0, 0, 5, 0, 0, 0] 
    },
    offset: 0
}
</code></pre>
<p>Now the length of the buffer is 24, even though we stored only 6 digits, and
there are extra zeros in the data array. What happened is that each of the u32
numbers is represented using multiples of u8 numbers. Now each number in the
array is padded and aligned. Neat isn't it?. </p>
<blockquote>
<p><strong>Tip</strong>. Try increasing the number of values used to create the buffer to see
what happens to the len. Also, try using numbers larger than 255 to see
how the number representation changes in the data array.</p>
</blockquote>
<p>Again, as a learning experience, you can use the raw pointer to access all the
elements from the buffer. However, since the buffer pointer is a <code>*const u8</code>
you need to cast it to a <code>*const u32</code>.</p>
<pre><pre class="playground"><code class="language-rust">use arrow::buffer::Buffer;
use arrow::datatypes::ToByteSlice;

fn main() {
    let buffer_u32 = Buffer::from(&amp;[0u32, 1, 2, 3, 4, 5].to_byte_slice());
    
    let ptr_32 = buffer_u32.as_ptr() as *const u32;
    unsafe {
        for i in 0..6 {
            println!(&quot;{}&quot;, *ptr_32.add(i));
        }
    }
}
</code></pre></pre>
<p>With your newly earned understanding of how a buffers works, lets start creating
Arrow arrays.</p>
<h1><a class="header" href="#the-array-data" id="the-array-data">The Array Data</a></h1>
<p>As we discussed before, an Arrow array is made out of several components and
the way these elements are stored will define the type of array that is being
created. In the Rust Arrow crate the
<a href="https://docs.rs/arrow/3.0.0/arrow/array/struct.ArrayData.html">ArrayData</a>
struct is the generic representation of the data stored in memory. All types of
arrays are made or represented using an atomic reference to an ArrayData. </p>
<p>Let us understand how this struct represents an Arrow array by creating one by
using the ArrayData::new implementation.</p>
<pre><pre class="playground"><code class="language-rust">use arrow::buffer::Buffer;
use arrow::array::ArrayData;
use arrow::datatypes::DataType;

fn main() {
    let buffer_u8 = Buffer::from(&amp;[0u8, 1, 2, 3, 4, 5]);
    let data = ArrayData::new(DataType::Int8, 6, None, None, 0, vec![buffer_u8], vec![]);

    println!(&quot;{:?}&quot;, data);
}
</code></pre></pre>
<p>If you print the previous code you should see the next output</p>
<pre><code class="language-json">ArrayData { 
    data_type: Int8,
    len: 6,
    null_count: 0,
    offset: 0, 
    buffers: [
        Buffer { 
            data: Bytes { 
                ptr: 0x20300849b00,
                len: 6, 
                data: [0, 1, 2, 3, 4, 5]
            },
            offset: 0
        }
    ],
    child_data: [],
    null_bitmap: None }
</code></pre>
<p>As you can see, to create the data (ArrayData::new) it was required to input the
datatype to be stored, the number of elements in the array, a validity null
buffer, an offset, a vector of buffers and child data. Each of these values is
used to define attributes and operations in the arrays. </p>
<p>Lets begin with the type of data. Each Arrow Array can store different
datatypes in memory as mentioned before. The available datatypes are defined
using the enum
<a href="https://docs.rs/arrow/3.0.0/arrow/datatypes/enum.DataType.html">DataType</a> and
it follows the Arrow specification on datatypes (see
<a href="https://github.com/apache/arrow/blob/master/format/Schema.fbs">Scehma.fbs</a>).
The selection of the datatype is very important because, as we saw in the
buffer chapter, the implementation needs to know what type of pointer to use
in order to access the vales stored in memory. </p>
<p>The next element is the length of values stored in the data. This value
indicates how many of the values available in the buffer will be considered in
the array.</p>
<blockquote>
<p><strong>Note</strong>. Remember that the buffer doesn't store the values using their
original datatype, instead it uses u8 types</p>
</blockquote>
<p>Therefore, the data array &quot;needs&quot; to know how many of the values from the
buffer it has to read. The offset has a similar use; it indicates the array
offset to start reading the data.</p>
<blockquote>
<p><strong>Tip</strong>. Change the len and offset values from the constructor from the
previous example to see what happens to the data array.</p>
</blockquote>
<p>The null bitmap and null count are used to indicate if there are null values
stored in the array and what their positions are. Have a read at this
<a href="https://github.com/apache/arrow/blob/master/docs/source/format/Columnar.rst#null-count">section</a>
of the Apache columnar format to understand how a null value is represented and
stored in the data array.</p>
<p>Also, during the construction of the data array we introduce an vector of
buffers to the constructor.  As we mentioned before some arrays required more
than one buffer to represent the data.  For example, an array of strings
requires a data buffer and an offset buffer (We'll see an example later).</p>
<p>The child data is used for nested arrays, such as a list array or a struct
array.  Each of them represent data collections that are composed of one or
more primitive arrays.</p>
<h1><a class="header" href="#primitive-arrays" id="primitive-arrays">Primitive Arrays</a></h1>
<p>A primitive array
(<a href="https://docs.rs/arrow/3.0.0/arrow/array/struct.PrimitiveArray.html">PrimitiveArray</a>)
is a type of array used to store a list of elements of the same type. It
includes fixed bit-width, variable-size, binary and null arrays.</p>
<p>Lets begin with an example of a primitive array and how the data looks like
when printed.</p>
<pre><pre class="playground"><code class="language-rust">use arrow::buffer::Buffer;
use arrow::array::{ArrayData, PrimitiveArray};
use arrow::datatypes::{DataType, Int32Type, ToByteSlice};

use std::sync::Arc;

fn main() {
    let buffer = Buffer::from(&amp;[0u32, 1, 2, 3, 4, 5].to_byte_slice());
    let data = ArrayData::new(DataType::Int32, 6, None, None, 0, vec![buffer], vec![]);

    let array: PrimitiveArray&lt;Int32Type&gt; = PrimitiveArray::&lt;Int32Type&gt;::from(Arc::new(data));
    println!(&quot;{:?}&quot;, array);
}
</code></pre></pre>
<p>This time you should see in your console output something like this</p>
<pre><code class="language-json">PrimitiveArray&lt;Int32&gt;
    [ 0,
      1,
      2,
      3,
      4,
      5,
    ]
</code></pre>
<p>It seems that this time the primitive array knows how to represent the data
that is stored in the buffer using the correct datatype. When we print the
array we no longer see zeros padding the data (easier for us humans to
understand, isn't it).</p>
<p>You may have noticed that we used the <strong>From</strong> trait in order to create the
array from the ArrayData. Lucky for us, in the Arrow crate there are several
ways to create arrays. </p>
<h2><a class="header" href="#the-array-builders" id="the-array-builders">The array builders</a></h2>
<p>Lets make our life simpler by using the constructors defined within the crate.
This constructors will do all the job of defining the buffers, data arrays and
datatypes. They will even help us define the validity buffer used to mark the
presence of null values.</p>
<p>For this example we will use an Int32Builder which is a type definition created
from
<a href="https://docs.rs/arrow/3.0.0/arrow/array/struct.PrimitiveBuilder.html">PrimitiveBuilder</a></p>
<pre><pre class="playground"><code class="language-rust">use arrow::array::Int32Builder;

fn main() {
    let mut primitive_array_builder = Int32Builder::new(20);

    primitive_array_builder.append_value(5).unwrap();
    primitive_array_builder.append_value(10000).unwrap();
    primitive_array_builder.append_value(2000000).unwrap();
    primitive_array_builder.append_null().unwrap();
    primitive_array_builder.append_slice(&amp;[1, 2, 3]).unwrap();
    primitive_array_builder.append_null().unwrap();
    primitive_array_builder
        .append_slice(&amp;(0..10).collect::&lt;Vec&lt;i32&gt;&gt;())
        .unwrap();

    let primitive_array = primitive_array_builder.finish();
    println!(&quot;{:?}&quot;, primitive_array);

}
</code></pre></pre>
<p>As you can see, now the array was created in a more organic way. We didn't
need to define all the elements that compose the array. This builder will let
us add as many values as we need (thanks to the
<a href="https://docs.rs/arrow/3.0.0/arrow/buffer/struct.MutableBuffer.html">MutableBuffer</a>
that is used by the constructor). We can add values, slices and nulls in one
go. When there are no more values to add, the builder will create a primitive
array that represents all the data stored within the data buffer. </p>
<p>It should be mentioned that once the builder finishes the array, it will clear
its memory and the builder can be used again to create another primitive array.</p>
<blockquote>
<p><strong>Note</strong>. The Arrow create also has
<a href="https://docs.rs/arrow/3.0.0/arrow/array/struct.BufferBuilder.html">BufferBuilders</a>
that behave like the array builders. They can be used to create buffers in a
dynamic way by adding values as needed. The finish buffer can be used to
create arrays of different types.</p>
</blockquote>
<h2><a class="header" href="#using-traits" id="using-traits">Using traits</a></h2>
<p>We can also create arrays by using vectors of elements. This is thanks to the
<strong>From</strong> trait implemented in the crate.</p>
<pre><pre class="playground"><code class="language-rust">use arrow::array::{PrimitiveArray, Int32Array};
use arrow::datatypes::{Date64Type, Time64MicrosecondType};

fn main() {
    // Creating an array from a vector of options
    let array = Int32Array::from(vec![Some(0), None, Some(2), None, Some(4)]);
    println!(&quot;{:?}&quot;, array);

    // Creating an array from a vector of Date64Types using the into method
    let date_array: PrimitiveArray&lt;Date64Type&gt; =
        vec![Some(1550902545147), None, Some(1550902545147)].into();
    println!(&quot;{:?}&quot;, date_array);

    // Creating an array from a vector of Date64Types using the from method
    let date_array: PrimitiveArray&lt;Date64Type&gt; = 
        PrimitiveArray::&lt;Date64Type&gt;::from(vec![Some(1550902545147), None, Some(1550902545147)]);
    println!(&quot;{:?}&quot;, date_array);

    let time_array: PrimitiveArray&lt;Time64MicrosecondType&gt; = 
        (0..100).collect::&lt;Vec&lt;i64&gt;&gt;().into();
    println!(&quot;{:?}&quot;, time_array);
}
</code></pre></pre>
<p>As you can see from these examples, it is relatively easy to create primitive
arrays to store data in memory. The create has a variety of methods to store
data in memory that follows the Arrow specification; all data is padded and
aligned. </p>
<p>Also, since all the arrays store an atomic reference to the buffers, they can be
shared between processes without copying the data. However, before we venture
into data sharing is important to see how to create nested structures using
primitive arrays and buffers.</p>
<h1><a class="header" href="#nested-arrays" id="nested-arrays">Nested arrays</a></h1>
<p>A primitive array is a useful struct to store values that have the same
datatype. However, if we only use primitive arrays it will be impossible to
represent complex data structures like datasets.  For this reason nested arrays
were introduced to the Apache Arrow specification, and in the Rust
implementation we have variable size lists, fixed size lists and structs.</p>
<h2><a class="header" href="#variable-size-lists" id="variable-size-lists">Variable size lists</a></h2>
<p>A variable size list is used to represent arrays that are made of variable size
lists. In Rust we have the next containers to create variable size lists:</p>
<ul>
<li><a href="https://docs.rs/arrow/3.0.0/arrow/array/type.ListArray.html">ListArray</a></li>
<li><a href="https://docs.rs/arrow/3.0.0/arrow/array/type.StringArray.html">StringArray</a></li>
<li><a href="https://docs.rs/arrow/3.0.0/arrow/array/type.BinaryArray.html">BinaryArray</a></li>
<li><a href="https://docs.rs/arrow/3.0.0/arrow/array/type.LargeListArray.html">LargeListArray</a></li>
<li><a href="https://docs.rs/arrow/3.0.0/arrow/array/type.LargeStringArray.html">LargeStringArray</a></li>
<li><a href="https://docs.rs/arrow/3.0.0/arrow/array/type.LargeBinaryArray.html">LargeBinaryArray</a></li>
</ul>
<blockquote>
<p><strong>Note</strong>. The difference between the Large and Normal arrays is the type of
datatype offset used to align the space between elements. In the normal
arrays the offset is represented using  a i32 and in the large list a i64 is
used.</p>
</blockquote>
<blockquote>
<p><strong>Tip</strong>. For more details on how a variable size list is defined and created, it is
recommended that you read this
<a href="https://github.com/apache/arrow/blob/master/docs/source/format/Columnar.rst#variable-size-list-layout">section</a>
of the Arrow columnar format. </p>
</blockquote>
<h3><a class="header" href="#listarray" id="listarray">ListArray</a></h3>
<p>Lets continue this section by creating a
<a href="https://docs.rs/arrow/3.0.0/arrow/array/type.ListArray.html">ListArray</a> to
show you how to create a variable size list and how it is represented in
memory.  For reference, we will use the next image to explain how a ListArray
is created and what is happening behind the scene.</p>
<p><img src="images/list.png" alt="Lists" /></p>
<p>In this example we are going to create a list that is made of three elements.
Each of these elements will me composed of one or more lists. For example,
element <strong>1</strong> will have 2 lists; element <strong>2</strong> will have 2 lists and 1 null;
and element <strong>3</strong> will have one list.</p>
<p>In general we can say that a nested array is made of a master data array and a
series of child data arrays. In the previous image the master array (<strong>a</strong>)
defines the general structure of the array, which in this case it has 3
elements, no nulls and has an offset buffer. The <a href="https://github.com/apache/arrow/blob/master/docs/source/format/Columnar.rst#variable-size-binary-layout">offset
buffer</a>
is used to calculate how many numbers or elements each of the inner lists have. </p>
<p>After the master data array comes the child array (<strong>b</strong>). The child array is
used to define the next nested level in the lists. In this case, the child
array has 6 elements, one null, and offset buffer and a values buffer. By using
this data representation one can nest as many lists as required. You only need
to define a next child and attach it to the ListArray. </p>
<p>With this description in mind, lets create the list array from the image with
the next code:</p>
<pre><pre class="playground"><code class="language-rust">use arrow::buffer::Buffer;
use arrow::array::{ArrayData, ListArray};
use arrow::datatypes::{DataType, Field, ToByteSlice};

fn main() {
    // First we create an ArrayData struct that will hold the values
    // from the array. These values are stored in one buffer aligned and padded.
    let value_data = ArrayData::builder(DataType::Int32)
        .len(10)
        .add_buffer(Buffer::from(&amp;[1, 2, 3, 4, 5, 6, 7, 8, 9, 10].to_byte_slice()))
        .build();

    // Second we define the offsets what will define the lists from the
    // child data(b)
    let value_offsets = Buffer::from(&amp;[0, 2, 4, 7, 7, 8, 10].to_byte_slice());

    // With the values and offset we can define the child data(b). The child
    // data represents the second level in the array. Notice the type for
    // the data array. It is made using the enum DataType::List indicating
    // that its a composite array
    let list_data_type = DataType::List(Box::new(Field::new(&quot;item&quot;, DataType::Int32, false)));
    let list_data = ArrayData::builder(list_data_type)
        .len(6)
        .add_buffer(value_offsets)
        .add_child_data(value_data)
        .null_bit_buffer(Buffer::from([0b00110111]))
        .build();
    
    // The last element is the master data array. This master data
    // array holds all the information required to define the ListArray
    let value_offsets = Buffer::from(&amp;[0, 2, 5, 6].to_byte_slice());
    let list_data_type = DataType::List(Box::new(Field::new(&quot;item&quot;, DataType::Int32, false)));
    let list_data = ArrayData::builder(list_data_type)
        .len(3)
        .add_buffer(value_offsets)
        .add_child_data(list_data)
        .build();

    // If you print the list_data you will only see the combination of 
    // buffers that compose the data array
    println!(&quot;{:?}&quot;, list_data);

    // We need to define a ListArray to be able to understand the composed
    // data that is inside the ListArray
    let list_array = ListArray::from(list_data);
    println!(&quot;{:?}&quot;, list_array);
}
</code></pre></pre>
<p>If you print the previous code you should see something like this (the output
was formated for this book):</p>
<pre><code class="language-json">ListArray [
    ListArray [
        PrimitiveArray&lt;Int32&gt; [ 1, 2, ],
        PrimitiveArray&lt;Int32&gt; [ 3, 4, ],
    ],
    ListArray [
        PrimitiveArray&lt;Int32&gt; [ 5, 6, 7, ],
        null,
        PrimitiveArray&lt;Int32&gt; [ 8, ],
    ],
    ListArray [
        PrimitiveArray&lt;Int32&gt; [ 9, 10, ],
    ],
]
</code></pre>
<p>As you can see, the resulting array is identical to the one described in the
previous image. Notice that by printing the ListArray we are able to see the
real representation of the struct and not the buffers (values represented using
u8).  Behind the scenes the ListArray is doing the steps <strong>(c)</strong> and <strong>(d)</strong>
from the image, which create the nested lists and show that data in the correct
format and structure. </p>
<p>A ListArray is very flexible and by following this procedure is possible to
create any type of nested lists.</p>
<h4><a class="header" href="#using-the-list-builder" id="using-the-list-builder">Using the List builder</a></h4>
<p>In order to make our life easier while creating a List Array we can use the
<a href="https://docs.rs/arrow/3.0.0/arrow/array/type.ListBuilder.html">ListBuilder</a>
and
<a href="https://docs.rs/arrow/3.0.0/arrow/array/type.LargeListBuilder.html">LargeListBuilder</a>.
By using these builders we no longer have to keep track of the three buffers
that compose the List Array. </p>
<p>The next example shows how a list array can be created by just inserting values
into it and selecting when a sub list starts and ends.</p>
<pre><pre class="playground"><code class="language-rust">use arrow::array::{Int32Builder, ListBuilder};

fn main() {
    // List array with builder
    let values_builder = Int32Builder::new(10);
    let mut builder = ListBuilder::new(values_builder);

    //  [[0, 1, 2], [3, 4, 5], [6, 7]]
    builder.values().append_value(0).unwrap();
    builder.values().append_value(1).unwrap();
    builder.values().append_value(2).unwrap();
    builder.append(true).unwrap();
    builder.values().append_value(3).unwrap();
    builder.values().append_value(4).unwrap();
    builder.values().append_value(5).unwrap();
    builder.append(true).unwrap();
    builder.values().append_value(6).unwrap();
    builder.values().append_value(7).unwrap();
    builder.append(true).unwrap();
    let list_array = builder.finish();

    println!(&quot;{:?}&quot;, list_array);
}
</code></pre></pre>
<h3><a class="header" href="#stringarray" id="stringarray">StringArray</a></h3>
<p>An String Array
(<a href="https://docs.rs/arrow/3.0.0/arrow/array/type.StringArray.html">StringArray</a>
and
<a href="https://docs.rs/arrow/3.0.0/arrow/array/type.LargeStringArray.html">LargeStringArray</a>)
is used to represent a list of strings stored using the arrow format. A
StringArray follows the same idea as a ListArray, the only difference is that
the values buffer is made of <strong>u8</strong>s that represents the letters or characters.</p>
<p>As an example lets create an StringArray that holds the next list:</p>
<pre><code class="language-json">array = [&quot;Hello&quot;, &quot;from&quot;, null, &quot;Apache&quot;, &quot;Arrow&quot;]
</code></pre>
<pre><pre class="playground"><code class="language-rust">use arrow::buffer::Buffer;
use arrow::array::{ArrayData, StringArray};
use arrow::datatypes::{DataType, ToByteSlice};

fn main() {
    // First we define the values that will represent the letters 
    // from the array
    let values: [u8; 20] = [
        b'h', b'e', b'l', b'l', b'o', b'f', b'r', b'o', b'm', 
        b'A', b'p', b'a', b'c', b'h', b'e', b'A', b'r', b'r', b'o', b'w', 
    ];

    // And the offset that represents how many characters are in each word
    let offsets: [i32; 6] = [0, 5, 9, 9, 15, 20];

    // By the way, notice the order each buffer is added to the ArrayData.
    // Each buffer is stored in a vector of buffers, so the only reference 
    // other constructors will have is the vector index.
    let array_data = ArrayData::builder(DataType::Utf8)
        .len(5)
        .add_buffer(Buffer::from(offsets.to_byte_slice()))
        .add_buffer(Buffer::from(&amp;values[..]))
        .null_bit_buffer(Buffer::from([0b00011011]))
        .build();
    let string_array = StringArray::from(array_data);

    println!(&quot;{:?}&quot;, string_array);
    println!(&quot;Value: {:?}&quot;, string_array.value(0));
    println!(&quot;Value: {:?}&quot;, string_array.value(1));
    println!(&quot;Value: {:?}&quot;, string_array.value(2));
}
</code></pre></pre>
<p>Your output should look similar to this:</p>
<pre><code class="language-json">StringArray
    [ &quot;hello&quot;, &quot;from&quot;, null, &quot;Apache&quot;, &quot;Arrow&quot;, ]
</code></pre>
<p>Notice how when we print the string array the strings are printed as the should
be and not as the u8 values stored in the buffer. This is thanks to the fact
that a StringArray &quot;knows&quot; the type of data it holds and thus can represent the
strings in the correct way. This can also be seen when the value() method is
used on the string. The returned value is the correct representation of the word
stored in the array.</p>
<blockquote>
<p><strong>Tip</strong>. From your code remove the null_bit_buffer method from the
constructor and see how the empty space is now represented.</p>
</blockquote>
<h4><a class="header" href="#using-the-string-builder" id="using-the-string-builder">Using the String builder</a></h4>
<p>Similar to the List Array, the construction of a String Array can become a bit
complicated if we do it from scratch. You would have to split and put together
all the letters from the list and then you would need to create an offset list
for the words, plus adding the validity buffer. This would take a lot of time
every time a new string array is required. For this reason, the
<a href="https://docs.rs/arrow/3.0.0/arrow/array/type.StringBuilder.html">StringBuilder</a>
and
<a href="https://docs.rs/arrow/3.0.0/arrow/array/type.LargeStringBuilder.html">LargeStringBuilder</a>
where created.</p>
<p>Lets create a new String Array using an String Builder.</p>
<pre><pre class="playground"><code class="language-rust">use arrow::array::StringBuilder;

fn main() {
    println!(&quot;Creating an String Array using builder&quot;);

    let mut builder = StringBuilder::new(10);
    builder.append_value(&quot;one&quot;).unwrap();
    builder.append_value(&quot;two&quot;).unwrap();
    builder.append_value(&quot;three&quot;).unwrap();
    builder.append_null().unwrap();
    builder.append_value(&quot;four&quot;).unwrap();

    let string_array = builder.finish();
    println!(&quot;{:?}&quot;, string_array);
}
</code></pre></pre>
<p>As you can see, the creation process is more streamlined an it feels more
natural. The builder will append the string and it will create the required
buffers automatically.</p>
<h2><a class="header" href="#struct-array" id="struct-array">Struct Array</a></h2>
<p><a href="https://docs.rs/arrow/3.0.0/arrow/array/struct.StructArray.html">StructArrays</a>
are used to represent mixed data, each being identified by a name and a
datatype.  As an example we have this array:</p>
<pre><code class="language-json">array = {
    &quot;a&quot;: [false, null, null, null, true],
    &quot;b&quot;: [null, 28, 42, null, null],
    &quot;c&quot;: [1, 2, 3, 4, 5]
}
</code></pre>
<p>As it can be seen, each element in the array is represented by a tuple made of
a name and an array of values. To construct an struct you will need to define
the data that represents each of the fields. In the next node we will construct
the previous array using the ArrayData builders we have been using.</p>
<pre><pre class="playground"><code class="language-rust">use arrow::buffer::Buffer;
use arrow::array::{ArrayData, StructArray};
use arrow::datatypes::{DataType, Field, ToByteSlice};

fn main() {
    // First we create all the base data that represents each of the elements
    // in the struct
    let boolean_data = ArrayData::builder(DataType::Boolean)
        .len(5)
        .add_buffer(Buffer::from([0b00010000]))
        .null_bit_buffer(Buffer::from([0b00010001]))
        .build();

    let int_data_b = ArrayData::builder(DataType::Int32)
        .len(5)
        .add_buffer(Buffer::from([0, 28, 42, 0, 0].to_byte_slice()))
        .null_bit_buffer(Buffer::from([0b00000110]))
        .build();

    let int_data_c = ArrayData::builder(DataType::Int32)
        .len(5)
        .add_buffer(Buffer::from([1, 2, 3, 4, 5].to_byte_slice()))
        .null_bit_buffer(Buffer::from([0b00011111]))
        .build();

    // The field types are used to indicate the type of data that each element
    // in the structarray has
    let mut field_types = vec![];
    field_types.push(Field::new(&quot;a&quot;, DataType::Boolean, false));
    field_types.push(Field::new(&quot;b&quot;, DataType::Int32, false));
    field_types.push(Field::new(&quot;c&quot;, DataType::Int32, false));

    let struct_array_data = ArrayData::builder(DataType::Struct(field_types))
        .len(5)
        .add_child_data(boolean_data.clone())
        .add_child_data(int_data_b.clone())
        .add_child_data(int_data_c.clone())
        .build();

    let struct_array = StructArray::from(struct_array_data);

    println!(&quot;{:?}&quot;, struct_array);
}
</code></pre></pre>
<p>The output from the code should look like this:</p>
<pre><code class="language-json">StructArray [
    -- child 0: &quot;a&quot; (Boolean)
    PrimitiveArray&lt;Boolean&gt; [ false, null, null, null, true, ]
    -- child 1: &quot;b&quot; (Int32)
    PrimitiveArray&lt;Int32&gt; [ null, 28, 42, null, null, ]
    -- child 2: &quot;c&quot; (Int32)
    PrimitiveArray&lt;Int32&gt; [ 1, 2, 3, 4, 5, ]
]
</code></pre>
<p>StructArrays can also be constructed using the StructArray::from helper, which
takes the underlying arrays and field types. In the next example we will
construct this struct using the From trait.</p>
<pre><code class="language-json">array = {
    &quot;b&quot;: [false, false, true, true],
    &quot;c&quot;: [42, 28, 19, 31]
}
</code></pre>
<pre><pre class="playground"><code class="language-rust">use arrow::array::{Array, BooleanArray, Int32Array, StructArray};
use arrow::datatypes::{DataType, Field};

use std::sync::Arc;

fn main() {
    let struct_array = StructArray::from(vec![
        (
            Field::new(&quot;b&quot;, DataType::Boolean, false),
            Arc::new(BooleanArray::from(vec![false, false, true, true])) as Arc&lt;dyn Array&gt;,
        ),
        (
            Field::new(&quot;c&quot;, DataType::Int32, false),
            Arc::new(Int32Array::from(vec![42, 28, 19, 31])),
        ),
    ]);

    println!(&quot;{:?}&quot;, struct_array);
}
</code></pre></pre>
<p>The output should look like this:</p>
<pre><code class="language-json">StructArray [
    -- child 0: &quot;b&quot; (Boolean)
    PrimitiveArray&lt;Boolean&gt; [ false, false, true, true, ]
    -- child 1: &quot;c&quot; (Int32)
    PrimitiveArray&lt;Int32&gt; [ 42, 28, 19, 31, ]
]
</code></pre>
<p>So, from these examples you can see that a struct is the perfect candidate to
represent in memory a dataframe. These dataframes can be shared, without
copying the data, among processes in order to read and process the data. You
could read a file and create a StructArray, and then pass a reference of such
array to another process for further analysis. And since all data is following
a columnar format, the dataframe is stored in memory in a very efficient way.</p>
<p>In the next chapter we are going to talk about the RecordBatch and how it can 
be used to share data between processes.</p>
<h1><a class="header" href="#the-recordbatch" id="the-recordbatch">The RecordBatch</a></h1>
<p>A
<a href="https://docs.rs/arrow/3.0.0/arrow/record_batch/struct.RecordBatch.html">RecordBatch</a>
is the last piece of the Arrow crate implementation. It could be said that is
the main goal of the whole Arrow specification; a structure that holds together
data and the metadata that describes it.</p>
<p>Have a look again at the image we saw in the introduction chapter.</p>
<p><img src="images/recordbatch.png" alt="Shared" /></p>
<p>Let us imagine that we are developing a service that, after executing a query,
returns formatted data with the requested information. For this service to be
efficient transferring data, the RecordBatch is an integral piece of the
process. Since each parcel of data contains all the information required to be
read and processed, any process receiving the data that <strong>&quot;speaks&quot;</strong> Arrow can
take the collected data and work on it immediately. There is no cost encoding or
decoding each parcel, it can be worked on it as soon as it is received.</p>
<p>We are going to cover how to transfer data in more detail in the <strong>IPC
chapter</strong>, but for the moment just keep in mind that a RecordBatch is one of the
keys for efficient data transfer.</p>
<h2><a class="header" href="#building-a-recordbatch" id="building-a-recordbatch">Building a RecordBatch</a></h2>
<p>We will begin our exploration into the RecordBatch with an small example. In
this example we are going to create a record that will contain two columns; an
index column and a fruits column. </p>
<pre><pre class="playground"><code class="language-rust">use std::sync::Arc;

use arrow::{
    array::{Int32Array, StringArray},
    datatypes::{DataType, Field, Schema},
    record_batch::RecordBatch,
};

fn main() {
    let schema = Schema::new(vec![
        Field::new(&quot;index&quot;, DataType::Int32, false),
        Field::new(&quot;fruits&quot;, DataType::Utf8, false),
    ]);

    let a = Int32Array::from(vec![1, 2, 3, 4, 5]);
    let b = StringArray::from(vec![&quot;apple&quot;, &quot;banana&quot;, &quot;pineapple&quot;, &quot;melon&quot;, &quot;pear&quot;]);

    let record_batch =
        RecordBatch::try_new(Arc::new(schema), vec![Arc::new(a), Arc::new(b)]).unwrap();

    println!(&quot;{:#?}&quot;, record_batch);
}
</code></pre></pre>
<p>If you run the example you should get something like this:</p>
<pre><code class="language-json">RecordBatch {
    schema: Schema {
        fields: [
            Field {
                name: &quot;index&quot;,
                data_type: Int32,
                nullable: false,
                dict_id: 0,
                dict_is_ordered: false,
                metadata: None,
            },
            Field {
                name: &quot;fruits&quot;,
                data_type: Utf8,
                nullable: false,
                dict_id: 0,
                dict_is_ordered: false,
                metadata: None,
            },
        ],
        metadata: {},
    },
    columns: [
        PrimitiveArray&lt;Int32&gt;
        [
          1,
          2,
          3,
          4,
          5,
        ],
        StringArray
        [
          &quot;apple&quot;,
          &quot;banana&quot;,
          &quot;pineapple&quot;,
          &quot;melon&quot;,
          &quot;pear&quot;,
        ],
    ],
}
</code></pre>
<h3><a class="header" href="#the-schema" id="the-schema">The Schema</a></h3>
<p>Now, let us inspect the pieces we used to create the RecordBatch. The first new
element that we will encounter is going to be the
<a href="https://docs.rs/arrow/3.0.0/arrow/datatypes/struct.Schema.html">Schema</a> struct.</p>
<pre><code class="language-rust ignore">    let schema = Schema::new(vec![
        Field::new(&quot;index&quot;, DataType::Int32, false),
        Field::new(&quot;fruits&quot;, DataType::Utf8, false),
    ]);
</code></pre>
<p>The Schema is used to describe the metadata that our little RecordBatch is going
to hold, and it is going to be our first source of information for the stored
data. Inside the Schema, each column has different properties that make them
unique, e.g. name, data type, etc. In order to encapsulate this information we
need to use the
<a href="https://docs.rs/arrow/3.0.0/arrow/datatypes/struct.Field.html">Field</a> struct.</p>
<p>Each Field in the Schema is responsible for holding the name of the column, the
data type, if the column is nullable, and additional metadata. The Field
metadata can be anything extra that we may want to store and that will be useful
when reading the data from the column.</p>
<p>The Schema struct also has the option to store extra metadata. For example, we
can store the number of rows a column has for future reference, or the name of
the file where the data comes from. Have a look to the next example:</p>
<pre><pre class="playground"><code class="language-rust">use std::collections::HashMap;
use arrow::datatypes::{DataType, Field, Schema};

fn main() {
    // Creating a schema with metadata
    let field_a = Field::new(&quot;a&quot;, DataType::Int64, false);
    let field_b = Field::new(&quot;b&quot;, DataType::Boolean, false);

    let mut metadata: HashMap&lt;String, String&gt; = HashMap::new();
    metadata.insert(&quot;row_count&quot;.to_string(), &quot;100&quot;.to_string());
    metadata.insert(&quot;file&quot;.to_string(), &quot;example.csv&quot;.to_string());

    let schema = Schema::new_with_metadata(vec![field_a, field_b], metadata);

    println!(&quot;{:#?}&quot;, schema);
}
</code></pre></pre>
<h3><a class="header" href="#the-recordbatch-data" id="the-recordbatch-data">The RecordBatch data</a></h3>
<p>The next piece of the RecordBatch is the data itself. This data comes from Arrow
arrays created using the methods we saw in previous sections.</p>
<pre><code class="language-rust ignore">    let a = Int32Array::from(vec![1, 2, 3, 4, 5]);
    let b = StringArray::from(vec![&quot;apple&quot;, &quot;banana&quot;, &quot;pineapple&quot;, &quot;melon&quot;, &quot;pear&quot;]);

    let record_batch =
        RecordBatch::try_new(Arc::new(schema), vec![Arc::new(a), Arc::new(b)]).unwrap();
</code></pre>
<p>From this snippet we can see that two columns are created using the From trait
implemented for Int32Array and StringArray.</p>
<p>The RecordBatch is finally constructed by passing the defined schema and a
vector of the created data. However, it should be noticed that we are using an
atomic reference (Arc) to the data and not the data itself, making the the
RecordBatch thread safe and the data zero copy. </p>
<h2><a class="header" href="#constructing-from-an-structarray" id="constructing-from-an-structarray">Constructing from an StructArray</a></h2>
<p>Probably those that have just read the previous chapter would have noticed that
a RecordBatch is very similar to an StructArray, and you are right. A
StructArray is a collection of Arrow arrays identified using Field structs. Both
share a similar data structure, and because of this similarity we can construct
a RecordBatch from them.</p>
<p>Have a look at the next code:</p>
<pre><pre class="playground"><code class="language-rust">use std::sync::Arc;

use arrow::{
    array::{ArrayRef, BooleanArray, Int32Array, StructArray},
    datatypes::{DataType, Field, Schema},
    record_batch::RecordBatch,
};

fn main() {
    let index = Arc::new(Int32Array::from(vec![0, 1, 2, 3]));
    let boolean_array = Arc::new(BooleanArray::from(vec![false, false, true, true]));
    let int_array = Arc::new(Int32Array::from(vec![42, 28, 19, 31]));

    let struct_array = StructArray::from(vec![
        (
            Field::new(&quot;index&quot;, DataType::Int32, false),
            index as ArrayRef,
        ),
        (
            Field::new(&quot;col_1&quot;, DataType::Int32, false),
            int_array as ArrayRef,
        ),
        (
            Field::new(&quot;col_2&quot;, DataType::Boolean, false),
            boolean_array as ArrayRef,
        ),
    ]);

    let record_batch = RecordBatch::from(&amp;struct_array);
    println!(&quot;{:#?}&quot;, record_batch);
}
</code></pre></pre>
<p>The output should look like this:</p>
<pre><code class="language-json">RecordBatch {
    schema: Schema {
        fields: [
            Field {
                name: &quot;index&quot;,
                data_type: Int32,
                nullable: false,
                dict_id: 0,
                dict_is_ordered: false,
                metadata: None,
            },
            Field {
                name: &quot;col_1&quot;,
                data_type: Int32,
                nullable: false,
                dict_id: 0,
                dict_is_ordered: false,
                metadata: None,
            },
            Field {
                name: &quot;col_2&quot;,
                data_type: Boolean,
                nullable: false,
                dict_id: 0,
                dict_is_ordered: false,
                metadata: None,
            },
        ],
        metadata: {},
    },
    columns: [
        PrimitiveArray&lt;Int32&gt;
        [
          0,
          1,
          2,
          3,
        ],
        PrimitiveArray&lt;Int32&gt;
        [
          42,
          28,
          19,
          31,
        ],
        BooleanArray
        [
          false,
          false,
          true,
          true,
        ],
    ],
}
</code></pre>
<p>As you can see we can construct a RecordBatch easily from the Struct array. This
is because the <strong>From</strong> trait is implemented for the RecordBatch. </p>
<h2><a class="header" href="#nested-structures" id="nested-structures">Nested structures</a></h2>
<p>To finish this section, we are going to see how to create a nested structure
using the tools that can be found within the RecorBatch struct. As you will see
the procedure is fairly straight forward, all we have to do is to define the
StructArray with all its columns and then add it to the schema as it is another
column of the RecordBatch.</p>
<p>The next snippet of code shows an example of how a nested structure could look
by introducing a Struct Array as one of the columns in the RecordBatch. The
resulting RecordBatch will have two columns; one called id and a another called
nested.</p>
<pre><pre class="playground"><code class="language-rust">use std::sync::Arc;

use arrow::{
    array::{Array, Float64Array, Int32Array, StringArray, StructArray},
    datatypes::{DataType, Field, Schema},
    record_batch::RecordBatch,
};

fn main() {
    let schema = Schema::new(vec![
        Field::new(&quot;id&quot;, DataType::Int32, false),
        Field::new(
            &quot;nested&quot;,
            DataType::Struct(vec![
                Field::new(&quot;a&quot;, DataType::Utf8, false),
                Field::new(&quot;b&quot;, DataType::Float64, false),
                Field::new(&quot;c&quot;, DataType::Float64, false),
            ]),
            false,
        ),
    ]);

    let id = Int32Array::from(vec![1, 2, 3, 4, 5]);

    let nested = StructArray::from(vec![
        (
            Field::new(&quot;a&quot;, DataType::Utf8, false),
            Arc::new(StringArray::from(vec![&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;])) as Arc&lt;dyn Array&gt;,
        ),
        (
            Field::new(&quot;b&quot;, DataType::Float64, false),
            Arc::new(Float64Array::from(vec![1.1, 2.2, 3.3, 4.4, 5.5])),
        ),
        (
            Field::new(&quot;c&quot;, DataType::Float64, false),
            Arc::new(Float64Array::from(vec![2.2, 3.3, 4.4, 5.5, 6.6])),
        ),
    ]);

    let record_batch =
        RecordBatch::try_new(Arc::new(schema), vec![Arc::new(id), Arc::new(nested)]).unwrap();

    println!(&quot;{:#?}&quot;, record_batch);
}

</code></pre></pre>
<p>After running the previous code you should see something like this:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>RecordBatch {
    schema: Schema {
        fields: [
            Field {
                name: &quot;id&quot;,
                data_type: Int32,
                nullable: false,
                dict_id: 0,
                dict_is_ordered: false,
                metadata: None,
            },
            Field {
                name: &quot;nested&quot;,
                data_type: Struct(
                    [
                        Field {
                            name: &quot;a&quot;,
                            data_type: Utf8,
                            nullable: false,
                            dict_id: 0,
                            dict_is_ordered: false,
                            metadata: None,
                        },
                        Field {
                            name: &quot;b&quot;,
                            data_type: Float64,
                            nullable: false,
                            dict_id: 0,
                            dict_is_ordered: false,
                            metadata: None,
                        },
                        Field {
                            name: &quot;c&quot;,
                            data_type: Float64,
                            nullable: false,
                            dict_id: 0,
                            dict_is_ordered: false,
                            metadata: None,
                        },
                    ],
                ),
                nullable: false,
                dict_id: 0,
                dict_is_ordered: false,
                metadata: None,
            },
        ],
        metadata: {},
    },
    columns: [
        PrimitiveArray&lt;Int32&gt;
        [
          1,
          2,
          3,
          4,
          5,
        ],
        StructArray
        [
        -- child 0: &quot;a&quot; (Utf8)
        StringArray
        [
          &quot;a&quot;,
          &quot;b&quot;,
          &quot;c&quot;,
          &quot;d&quot;,
          &quot;e&quot;,
        ]
        -- child 1: &quot;b&quot; (Float64)
        PrimitiveArray&lt;Float64&gt;
        [
          1.1,
          2.2,
          3.3,
          4.4,
          5.5,
        ]
        -- child 2: &quot;c&quot; (Float64)
        PrimitiveArray&lt;Float64&gt;
        [
          2.2,
          3.3,
          4.4,
          5.5,
          6.6,
        ]
        ],
    ],
}
<span class="boring">}
</span></code></pre></pre>
<p>As you can see, a RecordBatch is a very powerful structure that will let us put
together the information we want to process or share. We are going to keep using
it for different examples along this book.</p>
<h1><a class="header" href="#arrow-kernel--s" id="arrow-kernel--s">Arrow Kernel  s</a></h1>
<p>The arrow crate defines several operations that can be performed on arrays.
These include comparisons, aggregations, arithmetic operations, concatenations,
and more.</p>
<p>In this chapter we will see some of the most common operations that can be
performed on arrays.</p>
<h2><a class="header" href="#comparisons" id="comparisons">Comparisons</a></h2>
<h2><a class="header" href="#aggregations" id="aggregations">Aggregations</a></h2>
<h2><a class="header" href="#arithmetic-operations" id="arithmetic-operations">Arithmetic operations</a></h2>
<h2><a class="header" href="#concatenations" id="concatenations">Concatenations</a></h2>
<h1><a class="header" href="#arrow-ipc-interprocess-communication" id="arrow-ipc-interprocess-communication">Arrow IPC (Interprocess communication)</a></h1>
<p>In the previous section we have seen how to create Arrow arrays and batches
using the Rust API. This data could have been created from a running process
executing a query, and now it can be used for data analytics by any process
that requested it. Or probably, the generated piece of data is just part of a
bigger calculation that needs to be sent to other nodes to complete the
distributed operation. </p>
<p align="center">
  <img src="images/ipc.png">
</p>
<p>As you can see, the consumer of the data may not be in the same node and doesn't
have access to the memory and the Arrow data. For this, and many other reasons,
the producer needs a way to be able to send and share data with other processes.</p>
<p>Here is where the IPC specification starts to shine. In Arrow, the IPC is a
standard specification that all implementations of Arrow follow so they can
share data easily, and since all of them follow the same specification, there
is no need to serialize and deserialize from one representation to another.</p>
<p>In this section we are going to see how data can be sent between processes using
the available IPC API, either by storing the data in disk or by sending it
through the wire. We are going to start by describing the protocols that Arrow
uses to send information about the data and after that we are going to see some
examples to move and share Arrow data.</p>
<h1><a class="header" href="#detour-flatbuffers" id="detour-flatbuffers">Detour: Flatbuffers</a></h1>
<p>Before we dive into the IPC and how data is shared between multiple process
using Arrow's IPC, it is a good idea to get some understanding of what
Flatbuffers is and how it is used to serialize and deserialize data.</p>
<p>If you are familiar with data serialization, then feel free to skip this chapter.
We won't be discussing anything related to Arrow's IPC. This chapter will work as
a basic introduction for those that are not familiar with the process and it will
work as a foundation for understanding how data sharing works in Arrow.</p>
<h2><a class="header" href="#what-is-data-serialization" id="what-is-data-serialization">What is data serialization?</a></h2>
<p>Data serialization is the process of converting an object located in memory to a
representation that can be understood by other processes, and it is done by
converting the object to a series of bytes following some sort of contract that
all processes understand.</p>
<p>So, imagine that we have a process that needs to send data to another process,
it could be via disk or wire, it doesn't matter at the moment. What matters is
that the data has to be in a format that both understand. Now, if the data that
needs to be sent is an integer, like 20, then the producing server could send or
store &quot;10100&quot; and since the receiver knows that the bytes that is receiving
represent an integer then it can translate it back to 20. This processes was
trivial for an integer, but it can become cumbersome when there are complex
structures that need to be sent/receive.</p>
<p>This is why the serialization protocols where created. In Rust, we can use Serde
to serialize/deserialize almost any structure under the sun. However, Arrow IPC
uses Flatbuffers because it offers advantages that are beneficial to Arrow's IPC.</p>
<h2><a class="header" href="#flatbuffers" id="flatbuffers">Flatbuffers</a></h2>
<p>Flatbuffers is an open source
<a href="https://google.github.io/flatbuffers/index.html#flatbuffers_overview">project</a>
that was originally created for game development and other performance-critical
applications. It is fast, memory efficient, and flexible. However, one of its
best features is the fact that once an object has been serialized, the process
reading the data doesn't need to unpack it back, it can extract information
as soon as it has read the buffer.</p>
<p>Now, let us create a small example of data serialization by constructing a
generic struct and sharing the serialized data using a TCP connection using rust
std::net functions.</p>
<h3><a class="header" href="#installing-the-flatc-compiler" id="installing-the-flatc-compiler">Installing the flatc compiler</a></h3>
<p>Before you continue with the example you will need to install the flatc compiler. </p>
<p>If you are in OSX you can use:</p>
<pre><code>brew install flatbuffers
</code></pre>
<p>In ubuntu:</p>
<pre><code>sudo apt install -y flatbuffers-compiler
</code></pre>
<p>and in Windows</p>
<pre><code>choco install flatbuffers
</code></pre>
<p>Or you can also install it from the source following these
<a href="https://google.github.io/flatbuffers/flatbuffers_guide_building.html">instructions</a></p>
<h3><a class="header" href="#struct-serialization" id="struct-serialization">Struct serialization</a></h3>
<p>In this example we are going to define a very simple schema that could represent
a table, and that would be shared between different processes. For this example,
the table schema definition will have a list of fields and each of these fields
will be defined by a name and a type.</p>
<p>To start you will need to create the Flatbuffer objects that define our tables.
Write the next code in a file, lets call it Schema.fbs</p>
<pre><code>namespace MyStruct.Schema;

table Field {
    name:string;
    dtype:string;
}

table Schema {
    rows:long;
    fields:[Field];
}

root_type Schema;
</code></pre>
<p>As you can see we are creating two tables, Field and Schema. In Flatbuffers, a
<strong>table</strong> is the way to define an object with multiple elements, and they can be
composed of elements declared within the definition. In this case we are saying
that a the table could have a certain number of rows and a list of Fields. </p>
<blockquote>
<p><strong>Note</strong>. The Flatbuffers specification has multiple types of labels that can
be used to describe an object. For a detailed description of all the available
types, you should have a lot at <a href="https://google.github.io/flatbuffers/flatbuffers_guide_writing_schema.html">how to write a
schema</a>
from the FlatBuffers project.</p>
</blockquote>
<p>Now it is time to create the Rust file that will help us serialize and
deserialize the data. Run the next command using the file you created previously.</p>
<pre><code>flatc --rust Schema.fbs
</code></pre>
<p>This should generate a rust file with the same name as the file with the
Flatbuffer schema plus a <code>_generated.rs</code> postfix. Have a look at the file and
marvel at all the object definitions the <code>flatc</code> command created for us.</p>
<h4><a class="header" href="#creating-a-buffer" id="creating-a-buffer">Creating a buffer</a></h4>
<p>Now comes the interesting part, the serialization of the struct. Lets start by
creating a constructor using the flatbuffers crate:</p>
<pre><code class="language-rust ignore">let mut builder = flatbuffers::FlatBufferBuilder::new_with_capacity(1024);
</code></pre>
<p>This builder will be in charge of creating the correct conversions between
the rust objects and the Flatbuffer bytes representation.</p>
<p>Using the builder we can create two Field objects that will be added to
the mini table schema object we want to create:</p>
<pre><code class="language-rust ignore">let field_1_name = builder.create_string(&quot;col_1&quot;);
let field_1_dtype = builder.create_string(&quot;int&quot;);
let field_1 = Field::create(
    &amp;mut builder,
    &amp;FieldArgs {
        name: Some(field_1_name),
        dtype: Some(field_1_dtype),
    },
);

let field_2_name = builder.create_string(&quot;col_2&quot;);
let field_2_dtype = builder.create_string(&quot;int&quot;);
let field_2 = Field::create(
    &amp;mut builder,
    &amp;FieldArgs {
        name: Some(field_2_name),
        dtype: Some(field_2_dtype),
    },
);

let fields = builder.create_vector(&amp;[field_1, field_2]);
</code></pre>
<p>The builder has helped us to convert the strings that represent the name and
dtype for each of the fields that will be part of the main mini schema that
represents our table. Also, since the table schema is expecting these fields to
be stored in a Flatbuffer vector, we use the builder to create the required
object for the final table schema.</p>
<p>With all the information that defines the table schema converted we can create
the final object. The new schema objet will look like this:</p>
<pre><code class="language-rust ignore">let schema = Schema::create(
    &amp;mut builder,
    &amp;SchemaArgs {
        rows: 100,
        fields: Some(fields),
    },
);
</code></pre>
<p>The final stage of the serialization process is to create the bytes buffer
with the serialized object. This step is done in the next lines:</p>
<pre><code class="language-rust ignore">builder.finish(schema, None);
let buf = builder.finished_data();
</code></pre>
<p>Pat your self in the back, we are finally done. The object is serialized and it
is ready to be shared with other processes that require this information.</p>
<h4><a class="header" href="#deserializing-the-buffer" id="deserializing-the-buffer">Deserializing the buffer</a></h4>
<p>To finish this example we are going to recover the information in the buffer by
reading the buffer using the <code>root_as_schema</code> function that was generated with
the flatc compiler.</p>
<pre><code class="language-rust ignore">let recovered_schema = root_as_schema(buf).unwrap();
println!(&quot;{:?}&quot;, recovered_schema.rows());

let recovered_fields = recovered_schema.fields().unwrap();
for f in recovered_fields {
    println!(&quot;{:?}&quot;, f.name());
    println!(&quot;{:?}&quot;, f.dtype());
}
</code></pre>
<p>As you can see, the serialization process is quite straight forward using the
generated Flatbuffers objects, and the data deserialization is just as easy. The
bytes received in the buffer don't need to be unpacked, and their information
can be extracted soon as they are read. </p>
<p>In conclusion, in this small example we created a buffer of bytes that could
define a table with information about its fields. The resulting buffer is easy
to share with other processes, as it can be stored to disc or it can be shared
via a tcp stream. A similar process is done in the IPC Arrow module, obviously
with larger and more complex data structures. The schema information from the
RecordBatch is serialized to be written to any available stream, e.g. disk
writer or tcp writer. In the next chapter we are going to describe in more
detail how a RecordBatch is serialized to be consumed by other processes.</p>
<h1><a class="header" href="#reading-and-writing-files" id="reading-and-writing-files">Reading and writing files</a></h1>
<p>Most of the data required to do analysis is either stored in a file or a
database. For this reason it makes sense to cover the different methods
available in the Rust Arrow implementation to extract and save information
to a data file.</p>
<p>This section of the guide has three chapters discussing the available methods to
read data stored in files. The types of files that we are going to cover are
csv, json and parquet files. Hopefully by the end of this section you will
be familiar enough with the available interfaces to interact with files.</p>
<h1><a class="header" href="#reading-parquet-files" id="reading-parquet-files">Reading Parquet Files</a></h1>
<p>A <a href="https://parquet.apache.org">parquet</a> file is a columnar storage format that
is designed to efficiently store data in disk. Storing data using this format
presents several <a href="https://databricks.com/glossary/what-is-parquet">advantages</a>
and you are invited to have a look a them and conclude if these format could be of
use for your project. </p>
<p>In this chapter we are going to see how to open a parquet file using the
<a href="https://crates.io/crates/parquet">parquet</a> crate. The parquet crate is one of
several crates that forms part of the Rust arrow suite. The reader that we are
going to use from this crate reads a file in chunks and for each chunk it
creates a RecordBach that can be consumed. As you will see with the example,
these operations are fairly straight forward thanks to the parquet crate.</p>
<p>However, in order to make this chapter a bit more interesting, we are going to
create a Table struct that can be used to read and write parquet files. The
Table struct will implement some functions that will allow us to maintain
information in memory for further use and to extract specific values from them
either using an index or an iterator.</p>
<h2><a class="header" href="#the-data" id="the-data">The data</a></h2>
<p>The data that was used to test this code can be found from this
<a href="https://domohelp.domo.com/hc/en-us/articles/360043931814-Fun-Sample-DataSets">page</a>.
If you want to use the same dataset you will need to download the &quot;120 Years of
Olimpic History&quot; and convert it to a parquet file. The easiest way to do it is
by loading the csv file using pandas
(<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_parquet.html">pandas.read_parquet</a>)
and save it with pandas
(<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_parquet.html">df.to_parquet</a>).</p>
<blockquote>
<p><strong>Note</strong>. Keep in mind that the code that we are going to create can be used
to read any parquet file. So don't worry if you are unable to convert the
previously mentioned file. As long as you have a parquet file you are good to
go.</p>
</blockquote>
<h2><a class="header" href="#the-table-struct" id="the-table-struct">The Table struct</a></h2>
<p>The module that we are going to use to read the parquet is the
<a href="https://docs.rs/parquet/3.0.0/parquet/arrow/index.html">parquet::arrow</a>. This
module already defines a reader that can be used to extract the information in
chunks. However, since we want to keep the data in memory to use it for further
analysis, we will create and compose a struct called Table. </p>
<p>The Table struct will maintain a vector with the information extracted from the
parquet file and this data will be used to extract specific values from the
columns.</p>
<pre><pre class="playground"><code class="language-rust">use arrow::{
    record_batch::RecordBatch,
    datatypes::Schema,
};
use parquet::{
    arrow::{ArrowReader, ArrowWriter, ParquetFileArrowReader},
    file::reader::SerializedFileReader,
};
use std::sync::Arc;
use std::fs::File;
use std::path::Path;

// The Table struct. This object will represent the data read from the
// parquet files and it will be our entry point to any value in the file
pub struct Table {
    // We mantain a copy of the RecordBatch schema to keep handy the
    // file's metadata information.
    schema: Schema,
    data: Vec&lt;RecordBatch&gt;,
    rows: usize,
}

impl Table {
    pub fn read_parquet&lt;T: AsRef&lt;Path&gt;&gt;(path: T, chunk_size: usize) -&gt; Self {
        // Using the parquet Arrow reader to extract batches of data
        // from the file to keep them in memory
        let file = File::open(path).unwrap();
        let file_reader = SerializedFileReader::new(file).unwrap();
        let mut arrow_reader = ParquetFileArrowReader::new(Arc::new(file_reader));

        let schema = arrow_reader.get_schema().unwrap();
        let record_batch_reader = arrow_reader.get_record_reader(chunk_size).unwrap();
        let mut data: Vec&lt;RecordBatch&gt; = Vec::new();

        let mut rows = 0;
        for maybe_batch in record_batch_reader {
            let record_batch = maybe_batch.unwrap();
            rows += record_batch.num_rows();

            data.push(record_batch);
        }

        Self { schema, data, rows }
    }

    // Simple writer to store the table data into a parquet file
    pub fn to_parquet&lt;T: AsRef&lt;Path&gt;&gt;(&amp;self, path: T) {
        let file = File::create(path).unwrap();
        let mut writer = ArrowWriter::try_new(file, Arc::new(self.schema.clone()), None).unwrap();

        for batch in self.data.iter() {
            writer.write(&amp;batch).unwrap();
        }

        writer.close().unwrap();
    }

    pub fn schema(&amp;self) -&gt; &amp;Schema {
        &amp;self.schema
    }

    pub fn data(&amp;self) -&gt; &amp;Vec&lt;RecordBatch&gt; {
        &amp;self.data
    }

    pub fn rows(&amp;self) -&gt; usize {
        self.rows
    }
}

fn main() {
    let table = Table::read_parquet(&quot;data/olympics.parquet&quot;, 2000);
    println!(&quot;Number of rows: {}&quot;, table.rows())
}
</code></pre></pre>
<p>The most important functions from the struct are the <strong>read_parquet</strong> and
<strong>to_parquet</strong>. These represent the backbone used to manipulate parquet files.
In the <strong>read_parquet</strong> function we are reading the file in chunks or batches
using the
<a href="https://docs.rs/parquet/3.0.0/parquet/arrow/arrow_reader/struct.ParquetFileArrowReader.html">ParquetFileArrowReader</a>.
These batches, which are stored in a vector called data, will be our reference
for the next functions that we are going to implemented on Table. To write data
back to a parquet file we are using the
<a href="https://docs.rs/parquet/3.0.0/parquet/arrow/arrow_writer/struct.ArrowWriter.html">ArrowWriter</a>
struct, which writes the data to the desired path. As you can see, the parquet
crate has everything we need to read from and store data in parquet files. That
is very convenient and helpful.</p>
<p>To make the Table struct a bit useful for further work, we are also keeping a
copy of the RecordBatch schema in the table. This will make our life easier
whenever we want to extract the file's metadata. We also added some helper
functions in order to make the Table object a bit more useful.</p>
<p>Go ahead and compile this struct together with the main function to read and
write a parquet file. </p>
<p>Well, writing and reading data wasn't that hard. That's thanks to the great work
put into the parquet crate. Now, since we have created <strong>Table</strong> to read the
files, lets continue by giving it a bit more functionality to learn more about
the Arrow datatypes.</p>
<h2><a class="header" href="#getting-a-value" id="getting-a-value">Getting a value</a></h2>
<p>Here comes the interesting part of the Table struct; to extract a value from the
RecordBatches. One could be tempted to simply use the vector holding the
RecordBatches and try to read the values from there. Let say we could use an
index to select a RecordBatch from the vector and then using the RecordBatch
<a href="https://docs.rs/arrow/3.0.0/arrow/record_batch/struct.RecordBatch.html#method.column">column</a>
method we could select a column from the RecordBatch. With the desired column
available we could select any value from it. That sounds straight
forward, right?. If only Rust were that simple.</p>
<p>One thing that should be noted from the RecordBatch
<a href="https://docs.rs/arrow/3.0.0/arrow/record_batch/struct.RecordBatch.html#method.column">column</a>
method is that the return signature is <strong>&amp;ArrayRef</strong> which is an alias for and
<strong>Arc&lt;dyn Array&gt;</strong>. This means that the method returns a reference to an
object that implements dynamically the <strong>Array</strong> trait, not an explicit type of
Arrow array. This does make sense, since the return column can be of any Arrow
<a href="https://docs.rs/arrow/3.0.0/arrow/datatypes/index.html">datatype</a>, Rust needs
to know dynamically if the values read from the column are an integer, float,
string or a list. </p>
<p>That's why the <strong>Array</strong> trait is so useful in this case. It lets us work with
any array that implements the <strong>Array</strong> trait without worrying about its
specific type. However, this complicates our life because now we don't have an
specific type of array and thus we can not extract a value with its type from
the column. So, how are we going to access the real value from the columns?.</p>
<h2><a class="header" href="#enter-the-enums" id="enter-the-enums">Enter the enums</a></h2>
<p>One way in which we can access the data from any array that implements the
<strong>Array</strong> trait is by using the <strong>as_any</strong> method available to us via the
<strong>AsAny</strong> trait. The <strong>AsAny</strong> trait exposes the function <strong>dowcast_ref</strong> that,
as long as it is possible, downcasts this generic array to the specific array. 
We can do this for any column we would like to read data from. This approach
works but it is not the most flexible approach we can take.</p>
<p>Another thing we could do is to define an Enum that encapsulates all possible types
that could be found when reading the file. The advantage of using an enum this
way is that we can implement a unique function that converts or downcasts the
returned array into each of the possible Arrow arrays types.</p>
<p>In order to be able to downcast the Array to the desired array type we are going
to take advantage of the previously mentioned fact that the <strong>Array</strong> trait
implements the <strong>AsAny</strong> trait for all the Arrow array types. This means that we
will have to implement a function that downcasts an array based on the type of
data stored in it. To make this repetitive processes not so tedious, we are
going to help ourselves by writing these functions using a handy macro.</p>
<p>Have a look at the implementation of the enum ScalarValue</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use arrow::{
    array::{
        Array, ArrayRef, BooleanArray, Date32Array, Float32Array, Float64Array, Int16Array,
        Int32Array, Int64Array, Int8Array, LargeStringArray, ListArray, StringArray, UInt16Array,
        UInt32Array, UInt64Array, UInt8Array,
    },
    datatypes::{DataType, DateUnit, Field, Schema, TimeUnit},
    record_batch::RecordBatch,
};

#[derive(Debug, Clone, PartialEq)]
pub enum ScalarValue {
    Boolean(Option&lt;bool&gt;),
    Float32(Option&lt;f32&gt;),
    Float64(Option&lt;f64&gt;),
    Int8(Option&lt;i8&gt;),
    Int16(Option&lt;i16&gt;),
    Int32(Option&lt;i32&gt;),
    Int64(Option&lt;i64&gt;),
    UInt8(Option&lt;u8&gt;),
    UInt16(Option&lt;u16&gt;),
    UInt32(Option&lt;u32&gt;),
    UInt64(Option&lt;u64&gt;),
    Utf8(Option&lt;String&gt;),
    LargeUtf8(Option&lt;String&gt;),
    List(Option&lt;Vec&lt;ScalarValue&gt;&gt;, DataType),
    Date32(Option&lt;i32&gt;),
    TimeMicrosecond(Option&lt;i64&gt;),
    TimeNanosecond(Option&lt;i64&gt;),
}

// Helper macro that is used to create the function that downcasts
// an array to the correct type of array. This is done thanks to all
// the defined Arrow data types.
macro_rules! typed_cast {
    ($array:expr, $index:expr, $ARRAYTYPE:ident, $SCALAR:ident) =&gt; {{
        let array = $array.as_any().downcast_ref::&lt;$ARRAYTYPE&gt;().unwrap();
        ScalarValue::$SCALAR(match array.is_null($index) {
            true =&gt; None,
            false =&gt; Some(array.value($index).into()),
        })
    }};
}

impl ScalarValue {
    pub fn try_from_array(array: &amp;ArrayRef, index: usize) -&gt; Result&lt;Self, String&gt; {
        Ok(match array.data_type() {
            DataType::Boolean =&gt; typed_cast!(array, index, BooleanArray, Boolean),
            DataType::Float64 =&gt; typed_cast!(array, index, Float64Array, Float64),
            DataType::Float32 =&gt; typed_cast!(array, index, Float32Array, Float32),
            DataType::UInt64 =&gt; typed_cast!(array, index, UInt64Array, UInt64),
            DataType::UInt32 =&gt; typed_cast!(array, index, UInt32Array, UInt32),
            DataType::UInt16 =&gt; typed_cast!(array, index, UInt16Array, UInt16),
            DataType::UInt8 =&gt; typed_cast!(array, index, UInt8Array, UInt8),
            DataType::Int64 =&gt; typed_cast!(array, index, Int64Array, Int64),
            DataType::Int32 =&gt; typed_cast!(array, index, Int32Array, Int32),
            DataType::Int16 =&gt; typed_cast!(array, index, Int16Array, Int16),
            DataType::Int8 =&gt; typed_cast!(array, index, Int8Array, Int8),
            DataType::Utf8 =&gt; typed_cast!(array, index, StringArray, Utf8),
            DataType::LargeUtf8 =&gt; typed_cast!(array, index, LargeStringArray, LargeUtf8),
            DataType::List(nested_type) =&gt; {
                let list_array = array
                    .as_any()
                    .downcast_ref::&lt;ListArray&gt;()
                    .ok_or_else(|| &quot;Failed to downcast ListArray&quot;.to_string())?;
                let value = match list_array.is_null(index) {
                    true =&gt; None,
                    false =&gt; {
                        let nested_array = list_array.value(index);
                        let scalar_vec = (0..nested_array.len())
                            .map(|i| ScalarValue::try_from_array(&amp;nested_array, i))
                            .collect::&lt;Result&lt;Vec&lt;ScalarValue&gt;, String&gt;&gt;()?;
                        Some(scalar_vec)
                    }
                };
                ScalarValue::List(value, nested_type.data_type().clone())
            }
            DataType::Date32(DateUnit::Day) =&gt; {
                typed_cast!(array, index, Date32Array, Date32)
            }
            other =&gt; {
                return Err(format!(&quot;Downcast not available for type: {}&quot;, other));
            }
        })
    }
}
<span class="boring">}
</span></code></pre></pre>
<p>The <strong>try_from_array</strong> function uses an array, which until this point we only
know that is an object that implements dynamically the <strong>Array</strong> trait, and
downcasts it to the correct Arrow array type. This is done thanks to the options
defined in the ScalarValue enum and the Arrow implementation. Now, with this
enum under our belt we can implement the missing functions in the Table struct.</p>
<h2><a class="header" href="#the-complete-implementation" id="the-complete-implementation">The complete implementation</a></h2>
<p>With the ScalarValue enum defined, we can write the missing function from Table;
collect a value from a column. This missing function will extract a value from a
column using an index and returns it with the correct type. Also, since it may
be useful to have a way to loop though all the values in a column, we can
implement the iterator trait for a column.</p>
<p>The final implementation of all the code is presented next.</p>
<pre><pre class="playground"><code class="language-rust">use arrow::{
    array::{
        Array, ArrayRef, BooleanArray, Date32Array, Float32Array, Float64Array, Int16Array,
        Int32Array, Int64Array, Int8Array, LargeStringArray, ListArray, StringArray, UInt16Array,
        UInt32Array, UInt64Array, UInt8Array,
    },
    datatypes::{DataType, DateUnit, Field, Schema, TimeUnit},
    record_batch::RecordBatch,
};

use parquet::{
    arrow::{ArrowReader, ArrowWriter, ParquetFileArrowReader},
    file::reader::SerializedFileReader,
};

use std::fs::File;
use std::path::Path;
use std::sync::Arc;

#[derive(Debug, Clone, PartialEq)]
pub enum ScalarValue {
    Boolean(Option&lt;bool&gt;),
    Float32(Option&lt;f32&gt;),
    Float64(Option&lt;f64&gt;),
    Int8(Option&lt;i8&gt;),
    Int16(Option&lt;i16&gt;),
    Int32(Option&lt;i32&gt;),
    Int64(Option&lt;i64&gt;),
    UInt8(Option&lt;u8&gt;),
    UInt16(Option&lt;u16&gt;),
    UInt32(Option&lt;u32&gt;),
    UInt64(Option&lt;u64&gt;),
    Utf8(Option&lt;String&gt;),
    LargeUtf8(Option&lt;String&gt;),
    List(Option&lt;Vec&lt;ScalarValue&gt;&gt;, DataType),
    Date32(Option&lt;i32&gt;),
    TimeMicrosecond(Option&lt;i64&gt;),
    TimeNanosecond(Option&lt;i64&gt;),
}

macro_rules! typed_cast {
    ($array:expr, $index:expr, $ARRAYTYPE:ident, $SCALAR:ident) =&gt; {{
        let array = $array.as_any().downcast_ref::&lt;$ARRAYTYPE&gt;().unwrap();
        ScalarValue::$SCALAR(match array.is_null($index) {
            true =&gt; None,
            false =&gt; Some(array.value($index).into()),
        })
    }};
}

impl ScalarValue {
    pub fn try_from_array(array: &amp;ArrayRef, index: usize) -&gt; Result&lt;Self, String&gt; {
        Ok(match array.data_type() {
            DataType::Boolean =&gt; typed_cast!(array, index, BooleanArray, Boolean),
            DataType::Float64 =&gt; typed_cast!(array, index, Float64Array, Float64),
            DataType::Float32 =&gt; typed_cast!(array, index, Float32Array, Float32),
            DataType::UInt64 =&gt; typed_cast!(array, index, UInt64Array, UInt64),
            DataType::UInt32 =&gt; typed_cast!(array, index, UInt32Array, UInt32),
            DataType::UInt16 =&gt; typed_cast!(array, index, UInt16Array, UInt16),
            DataType::UInt8 =&gt; typed_cast!(array, index, UInt8Array, UInt8),
            DataType::Int64 =&gt; typed_cast!(array, index, Int64Array, Int64),
            DataType::Int32 =&gt; typed_cast!(array, index, Int32Array, Int32),
            DataType::Int16 =&gt; typed_cast!(array, index, Int16Array, Int16),
            DataType::Int8 =&gt; typed_cast!(array, index, Int8Array, Int8),
            DataType::Utf8 =&gt; typed_cast!(array, index, StringArray, Utf8),
            DataType::LargeUtf8 =&gt; typed_cast!(array, index, LargeStringArray, LargeUtf8),
            DataType::List(nested_type) =&gt; {
                let list_array = array
                    .as_any()
                    .downcast_ref::&lt;ListArray&gt;()
                    .ok_or_else(|| &quot;Failed to downcast ListArray&quot;.to_string())?;
                let value = match list_array.is_null(index) {
                    true =&gt; None,
                    false =&gt; {
                        let nested_array = list_array.value(index);
                        let scalar_vec = (0..nested_array.len())
                            .map(|i| ScalarValue::try_from_array(&amp;nested_array, i))
                            .collect::&lt;Result&lt;Vec&lt;ScalarValue&gt;, String&gt;&gt;()?;
                        Some(scalar_vec)
                    }
                };
                ScalarValue::List(value, nested_type.data_type().clone())
            }
            DataType::Date32(DateUnit::Day) =&gt; {
                typed_cast!(array, index, Date32Array, Date32)
            }
            other =&gt; {
                return Err(format!(&quot;Downcast not available for type: {}&quot;, other));
            }
        })
    }
}

pub struct Table {
    schema: Schema,
    data: Vec&lt;RecordBatch&gt;,
    rows: usize,
    // We keep the batch chunk size to calculate a relative index
    // to access the information from the data vector
    chunk_size: usize,
}

impl Table {
    pub fn read_parquet&lt;T: AsRef&lt;Path&gt;&gt;(path: T, chunk_size: usize) -&gt; Self {
        let file = File::open(path).unwrap();
        let file_reader = SerializedFileReader::new(file).unwrap();
        let mut arrow_reader = ParquetFileArrowReader::new(Arc::new(file_reader));

        let schema = arrow_reader.get_schema().unwrap();
        let record_batch_reader = arrow_reader.get_record_reader(chunk_size).unwrap();
        let mut data: Vec&lt;RecordBatch&gt; = Vec::new();

        let mut rows = 0;
        for maybe_batch in record_batch_reader {
            let record_batch = maybe_batch.unwrap();
            rows += record_batch.num_rows();

            data.push(record_batch);
        }

        Self {
            schema,
            data,
            rows,
            chunk_size,
        }
    }

    pub fn to_parquet&lt;T: AsRef&lt;Path&gt;&gt;(&amp;self, path: T) {
        let file = File::create(path).unwrap();
        let mut writer = ArrowWriter::try_new(file, Arc::new(self.schema.clone()), None).unwrap();

        for batch in self.data.iter() {
            writer.write(&amp;batch).unwrap();
        }

        writer.close().unwrap();
    }

    pub fn schema(&amp;self) -&gt; &amp;Schema {
        &amp;self.schema
    }

    pub fn data(&amp;self) -&gt; &amp;Vec&lt;RecordBatch&gt; {
        &amp;self.data
    }

    pub fn rows(&amp;self) -&gt; usize {
        self.rows
    }

    // Function to get a value from a column in the table
    // The function will search in the batches from the data
    // vector and returns the selected value with its correct
    // datatype
    pub fn value(&amp;self, column: usize, index: usize) -&gt; Option&lt;ScalarValue&gt; {
        if column &gt;= self.schema.fields().len() {
            return None;
        }

        let batch = index / self.chunk_size;
        if batch &gt;= self.data.len() {
            return None;
        }

        let array = self.data[batch].column(column);
        let index_in_batch = index % self.chunk_size;

        ScalarValue::try_from_array(array, index_in_batch).ok()
    }


    pub fn column_iterator(&amp;self, column: usize) -&gt; ColumnIterator {
        ColumnIterator::new(column, &amp;self.data)
    }
}

// Iterator to loop through all the values in a column using
// as return value a ScalarValue
pub struct ColumnIterator&lt;'iter&gt; {
    column: usize,
    data: &amp;'iter [RecordBatch],
    index: usize,
    batch: usize,
}

impl&lt;'iter&gt; ColumnIterator&lt;'iter&gt; {
    pub fn new(column: usize, data: &amp;'iter [RecordBatch]) -&gt; Self {
        Self {
            column,
            data,
            index: 0,
            batch: 0,
        }
    }
}

impl&lt;'iter&gt; Iterator for ColumnIterator&lt;'iter&gt; {
    type Item = ScalarValue;

    fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; {
        let records = self.data[self.batch].column(self.column).len();

        let (next_record, next_batch) = if self.index + 1 &gt;= records {
            (0, self.batch + 1)
        } else {
            (self.index + 1, self.batch)
        };

        if next_batch &gt;= self.data.len() {
            return None;
        }

        let array = self.data[self.batch].column(self.column);

        let value = ScalarValue::try_from_array(array, self.index).ok();

        self.index = next_record;
        self.batch = next_batch;

        value
    }
}

fn main() {
    let table = Table::read_parquet(&quot;data/olympics.parquet&quot;, 2000);

    let col_iter = table.column_iterator(0);

    for val in col_iter {
        if let ScalarValue::Int64(res) = val {
            println!(&quot;{:?}&quot;, res);
        }
    }
}
</code></pre></pre>
<p>It should be noted that without the definition of the <strong>ScalarValue</strong> enum, it
would have been impossible to keep the return value generic for any data type
defined in the Arrow implementation. There was no way to create an specific
Arrow datatype return value for this function. Well, maybe we could have done it
with a new trait, but we would have had to do more work than the one we just
did. The enum has saved our day.</p>
<p>Have fun compiling the code and testing reading and writing different
parquet files. </p>
<h2><a class="header" href="#conclusion" id="conclusion">Conclusion</a></h2>
<p>From this and the previous examples that we saw in this section, we hope you get
and idea of how you could use the Arrow implementation to read data from
different type of files and use them for data analysis and calculations. As you
can see, the Rust Arrow suite already has several methods and structs that make
these operations simple to implement.</p>
<p>One thing that should be mention is that the implementation of the ScalarValue
enum is a simplification of the approach used in the
<a href="https://docs.rs/datafusion/3.0.0/datafusion/">Datafusion</a> crate. The objective
of Datafusion is to create an interface for doing complex data operations using
Arrow as the data backbone. It implements a
<a href="https://docs.rs/datafusion/3.0.0/datafusion/dataframe/trait.DataFrame.html">DataFrame</a>
which is a more advanced and complex version of <strong>Table</strong> struct we just created
in this example. It aims to become a Pandas analogue in Rust. We are
going to discuss Datafusion in future chapters but before that, we are going to
talk about IPC (interprocess communication) and how Arrow is used to share data
between processes. </p>
<h1><a class="header" href="#contributors" id="contributors">Contributors</a></h1>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
